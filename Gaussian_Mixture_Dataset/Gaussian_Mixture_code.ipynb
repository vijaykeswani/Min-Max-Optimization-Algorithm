{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was used for the simulations in Figure 3 and Appendix E.6 of our paper. (This code is for our algorithm, GDA, and Unrolled GANs, on the four Gaussian mixture dataset.)\n",
    "\n",
    "For our algorithm, set the hyperparamters unrolling_steps=0, disc_steps = 6, rate = 4. For GDA with 1 discriminator step, set the hyperparamters unrolling_steps=0, disc_steps = 1, rate = 1.  For GDA with 6 discriminator steps, set  the hyperparamters unrolling_steps=0, disc_steps = 6, rate = 1.  For Unrolled GANs, set the hyperparamters unrolling_steps=6, disc_steps = 1, rate = 1. For OMD, use the optimAdam optimizer and disc_steps = 1, rate = 1.\n",
    "\n",
    "(Note that the part of our algorithm which saves the generator and discriminator weights is not implemented efficiently in tensorflow the particular code we give here.  It instead copies the weights into numpy and then back into tensorflow, which is a very innefficient process.  For this reason, each iteration of our algorithm runs very slowly in this code.  For an efficient implementation of our algorithm, with the weights \"saved\" directly in tensorflow, see instead our code for MNIST or CIFAR-10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Warning: moviepy not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf\n",
    "ds = tf.contrib.distributions\n",
    "slim = tf.contrib.slim\n",
    "        \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "try:\n",
    "    from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "    import moviepy.editor as mpy\n",
    "    generate_movie = True\n",
    "except:\n",
    "    print(\"Warning: moviepy not found.\")\n",
    "    generate_movie = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_graph_replace = tf.contrib.graph_editor.graph_replace\n",
    "\n",
    "def remove_original_op_attributes(graph):\n",
    "    \"\"\"Remove _original_op attribute from all operations in a graph.\"\"\"\n",
    "    for op in graph.get_operations():\n",
    "        op._original_op = None\n",
    "        \n",
    "def graph_replace(*args, **kwargs):\n",
    "    \"\"\"Monkey patch graph_replace so that it works with TF 1.0\"\"\"\n",
    "    remove_original_op_attributes(tf.get_default_graph())\n",
    "    return _graph_replace(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_update_dict(update_ops):\n",
    "    \"\"\"Extract variables and their new values from Assign and AssignAdd ops.\n",
    "    \n",
    "    Args:\n",
    "        update_ops: list of Assign and AssignAdd ops, typically computed using Keras' opt.get_updates()\n",
    "\n",
    "    Returns:\n",
    "        dict mapping from variable values to their updated value\n",
    "    \"\"\"\n",
    "    name_to_var = {v.name: v for v in tf.global_variables()}\n",
    "    updates = OrderedDict()\n",
    "    for update in update_ops:\n",
    "        var_name = update.op.inputs[0].name\n",
    "        var = name_to_var[var_name]\n",
    "        value = update.op.inputs[1]\n",
    "        if update.op.type == 'Assign':\n",
    "            updates[var.value()] = value\n",
    "        elif update.op.type == 'AssignAdd':\n",
    "            updates[var.value()] = var + value\n",
    "        #else:\n",
    "        #    raise ValueError(\"Update op type (%s) must be of type Assign or AssignAdd\"%update_op.op.type)\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mog(batch_size, n_mixture=5, std=0.01, radius=1.0):\n",
    "    thetas = np.linspace(0, 2 * np.pi, n_mixture)\n",
    "    xs, ys = radius * np.sin(thetas), radius * np.cos(thetas)\n",
    "    cat = ds.Categorical(tf.zeros(n_mixture))\n",
    "    comps = [ds.MultivariateNormalDiag([xi, yi], [std, std]) for xi, yi in zip(xs.ravel(), ys.ravel())]\n",
    "    data = ds.Mixture(cat, comps)\n",
    "    return data.sample(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and discriminator architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, output_dim=2, n_hidden=128, n_layer=2):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        h = slim.stack(z, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)#tf.nn.tanh)\n",
    "        x = slim.fully_connected(h, output_dim, activation_fn=None)\n",
    "    return x\n",
    "\n",
    "def discriminator(x, n_hidden=128, n_layer=2, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        h = slim.stack(x, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)#tf.nn.tanh)\n",
    "        log_d = slim.fully_connected(h, 1, activation_fn=None)\n",
    "    return log_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=512,\n",
    "    disc_learning_rate=1e-4,\n",
    "    gen_learning_rate=1e-3,\n",
    "    beta1=0.5,\n",
    "    epsilon=1e-8,\n",
    "    max_iter=1500,\n",
    "    viz_every=100,\n",
    "    z_dim=256,\n",
    "    x_dim=2,\n",
    "    unrolling_steps=0,\n",
    "    disc_steps = 6,\n",
    "    rate = 4,\n",
    "    type=\"cross\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GDA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_setup():\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    data = sample_mog(params['batch_size'])\n",
    "    noise = ds.Normal(tf.zeros(params['z_dim']), \n",
    "                      tf.ones(params['z_dim'])).sample(params['batch_size'])\n",
    "    # Construct generator and discriminator nets\n",
    "    with slim.arg_scope([slim.fully_connected], weights_initializer=tf.orthogonal_initializer(gain=0.8)):\n",
    "        samples = generator(noise, output_dim=params['x_dim'])\n",
    "        real_score = discriminator(data)\n",
    "        fake_score = discriminator(samples, reuse=True)\n",
    "\n",
    "    # Saddle objective    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=real_score, labels=tf.ones_like(real_score)) +\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_score, labels=tf.zeros_like(fake_score)))\n",
    "\n",
    "    gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "    #disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "\n",
    "    # Vanilla discriminator update: for loop is to take many discriminator steps\n",
    "    #for disc_steps  in range(0, 10):\n",
    "    disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "    d_opt = Adam(lr=params['disc_learning_rate'], beta_1=params['beta1'], epsilon=params['epsilon'])\n",
    "    updates = d_opt.get_updates(disc_vars, [], loss)\n",
    "    d_train_op = tf.group(*updates, name=\"d_train_op\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Unroll optimization of the discrimiantor\n",
    "    if params['unrolling_steps'] > 0:\n",
    "        # Get dictionary mapping from variables to their update value after one optimization step\n",
    "        update_dict = extract_update_dict(updates)\n",
    "        cur_update_dict = update_dict\n",
    "        for i in range(params['unrolling_steps'] - 1):\n",
    "            # Compute variable updates given the previous iteration's updated variable\n",
    "            cur_update_dict = graph_replace(update_dict, cur_update_dict)\n",
    "        # Final unrolled loss uses the parameters at the last time step\n",
    "        unrolled_loss = graph_replace(loss, cur_update_dict)\n",
    "    else:\n",
    "        unrolled_loss = loss\n",
    "\n",
    "    # Optimize the generator on the unrolled loss\n",
    "    g_train_opt = tf.train.AdamOptimizer(params['gen_learning_rate'], beta1=params['beta1'], epsilon=params['epsilon'])\n",
    "    g_train_op = g_train_opt.minimize(-unrolled_loss, var_list=gen_vars)\n",
    "\n",
    "    # Optimize the Vanilla generator on the non-unrolled loss\n",
    "    g_train_op_vanilla = g_train_opt.minimize(-loss, var_list=gen_vars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #make a copy of the weights\n",
    "    disc_vars_copy = [tf.identity(disc_vars[0]), tf.identity(disc_vars[1]), tf.identity(disc_vars[2]), tf.identity(disc_vars[3]), tf.identity(disc_vars[4]), tf.identity(disc_vars[5])]\n",
    "    gen_vars_copy = [tf.identity(gen_vars[0]), tf.identity(gen_vars[1]), tf.identity(gen_vars[2]), tf.identity(gen_vars[3]), tf.identity(gen_vars[4]), tf.identity(gen_vars[5])]\n",
    "    \n",
    "    return loss, unrolled_loss, g_train_op, d_train_op, samples, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### GDA algorithm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "xmax = 3\n",
    "fs = []\n",
    "frames = []\n",
    "np_samples = []\n",
    "n_batches_viz = 100\n",
    "viz_every = params['viz_every']    \n",
    "\n",
    "reps = 20\n",
    "for _ in tqdm(range(reps)):\n",
    "    loss, unrolled_loss, g_train_op, d_train_op, samples, data = initialize_setup()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #initialize at a very high loss value (to ensure that the first step is accepted)\n",
    "    f = [10000000]\n",
    "    points = []\n",
    "    for i in tqdm(range(params['max_iter'])):\n",
    "        \n",
    "        disc_steps = 1\n",
    "        rate = params['rate']\n",
    "        \n",
    "        #take one generator gradient update and one discriminator gradient update\n",
    "        f_new, _, _ = sess.run([[unrolled_loss, loss], g_train_op, d_train_op])\n",
    "\n",
    "        #take the additional discriminator gradient updates\n",
    "        for disc_steps  in range(0, disc_steps-1):\n",
    "             f_new, _ = sess.run([[loss], d_train_op])\n",
    "\n",
    "        f = f_new\n",
    "\n",
    "        if i % viz_every == 0:\n",
    "            np_samples.append(np.vstack([sess.run(samples) for _ in range(n_batches_viz)]))\n",
    "            xx, yy = sess.run([samples, data])\n",
    "            points.append((xx, yy))\n",
    "            xx, yy = sess.run([samples, data])\n",
    "            fig = figure(figsize=(5,5))\n",
    "            plt.scatter(yy[:, 0], yy[:, 1], c='r', edgecolor='none')\n",
    "            plt.scatter(xx[:, 0], xx[:, 1], edgecolor='none')\n",
    "            plt.axis('off')\n",
    "            plt.xlim(-1.5, 1.5)\n",
    "            plt.ylim(-1.5, 1.5)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Minimax algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "xmax = 3\n",
    "fs = []\n",
    "frames = []\n",
    "np_samples = []\n",
    "n_batches_viz = 100\n",
    "viz_every = params['viz_every']\n",
    "\n",
    "reps = 20\n",
    "ps = []\n",
    "for _ in tqdm(range(reps)):\n",
    "\n",
    "    loss, unrolled_loss, g_train_op, d_train_op, samples, data = initialize_setup()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #initialize at a very high loss value (to ensure that the first step is accepted)\n",
    "    f = [10000000]\n",
    "\n",
    "    for i in (range(params['max_iter'])):\n",
    "        disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "        gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "\n",
    "        fs.append(f)\n",
    "        disc_steps = params[\"disc_steps\"]\n",
    "        rate = params['rate']\n",
    "\n",
    "        #Accept/reject step\n",
    "        if i%rate != 0:\n",
    "\n",
    "            #copy the current generator and discriminator weights in case the algorithm decides to go back to them later\n",
    "            gen_vars_old = sess.run(gen_vars)\n",
    "            disc_vars_old = sess.run(disc_vars)\n",
    "\n",
    "            #take one generator gradient update and one discriminator gradient update\n",
    "            f_new, _, _ = sess.run([[unrolled_loss, loss], g_train_op, d_train_op])\n",
    "\n",
    "\n",
    "            #take the additional discriminator updates\n",
    "            for disc_steps  in range(0, disc_steps-1):\n",
    "                f_new, _ = sess.run([[loss], d_train_op])\n",
    "\n",
    "\n",
    "            #Keep the weights which lead to the smaller loss    \n",
    "            if f_new > f:\n",
    "                print(\"reject\")\n",
    "                #If the old weights are better, replace the new weights with the old weights\n",
    "                sess.run(tf.assign(gen_vars[0], gen_vars_old[0]))\n",
    "                sess.run(tf.assign(gen_vars[1], gen_vars_old[1]))   \n",
    "                sess.run(tf.assign(gen_vars[2], gen_vars_old[2])) \n",
    "                sess.run(tf.assign(gen_vars[3], gen_vars_old[3]))\n",
    "                sess.run(tf.assign(gen_vars[4], gen_vars_old[4]))    \n",
    "                sess.run(tf.assign(gen_vars[5], gen_vars_old[5])) \n",
    "            else:\n",
    "                print(\"accept\")\n",
    "            f = f_new\n",
    "\n",
    "        else:\n",
    "            #take one generator gradient update and one discriminator gradient update\n",
    "            f_new, _, _ = sess.run([[unrolled_loss, loss], g_train_op, d_train_op])\n",
    "\n",
    "            #take the additional discriminator gradient updates\n",
    "            for disc_steps  in range(0, disc_steps-1):\n",
    "                 f_new, _ = sess.run([[loss], d_train_op])\n",
    "\n",
    "            f = f_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #plot the output\n",
    "        if i % viz_every == 0:\n",
    "            np_samples.append(np.vstack([sess.run(samples) for _ in range(n_batches_viz)]))\n",
    "            xx, yy = sess.run([samples, data])\n",
    "            fig = figure(figsize=(5,5))\n",
    "            scatter(yy[:, 0], yy[:, 1], c='r', edgecolor='none')\n",
    "            scatter(xx[:, 0], xx[:, 1], edgecolor='none')\n",
    "            axis('off')\n",
    "            plt.xlim(-1.5, 1.5)\n",
    "            plt.ylim(-1.5, 1.5)\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using OMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimAdam import *\n",
    "\n",
    "def initialize_omd_setup():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    data = sample_mog(params['batch_size'])\n",
    "\n",
    "    noise = ds.Normal(tf.zeros(params['z_dim']), \n",
    "                      tf.ones(params['z_dim'])).sample(params['batch_size'])\n",
    "    # Construct generator and discriminator nets\n",
    "    with slim.arg_scope([slim.fully_connected], weights_initializer=tf.orthogonal_initializer(gain=0.8)):\n",
    "        samples = generator(noise, output_dim=params['x_dim'])\n",
    "        real_score = discriminator(data)\n",
    "        fake_score = discriminator(samples, reuse=True)\n",
    "\n",
    "    # Wasserstein loss    \n",
    "    loss = tf.reduce_mean(fake_score - real_score)\n",
    "\n",
    "    gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "\n",
    "    # Vanilla discriminator update: for loop is to take many discriminator steps\n",
    "    disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "    d_opt = optimAdam(lr=params['disc_learning_rate'], beta_1=params['beta1'], epsilon=params['epsilon'])\n",
    "    updates = d_opt.get_updates(disc_vars, [], loss)\n",
    "    d_train_op = tf.group(*updates, name=\"d_train_op\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Unroll optimization of the discrimiantor\n",
    "    if params['unrolling_steps'] > 0:\n",
    "        # Get dictionary mapping from variables to their update value after one optimization step\n",
    "        update_dict = extract_update_dict(updates)\n",
    "        cur_update_dict = update_dict\n",
    "        for i in range(params['unrolling_steps'] - 1):\n",
    "            # Compute variable updates given the previous iteration's updated variable\n",
    "            cur_update_dict = graph_replace(update_dict, cur_update_dict)\n",
    "        # Final unrolled loss uses the parameters at the last time step\n",
    "        unrolled_loss = graph_replace(loss, cur_update_dict)\n",
    "    else:\n",
    "        unrolled_loss = loss\n",
    "\n",
    "    # # Optimize the generator on the unrolled loss\n",
    "    \n",
    "    g_opt = optimAdam(lr=params['gen_learning_rate'], beta_1=params['beta1'], epsilon=params['epsilon'])\n",
    "    updates = g_opt.get_updates(gen_vars, [], -loss)\n",
    "    g_train_op = tf.group(*updates, name=\"d_train_op\")\n",
    "\n",
    "\n",
    "    return loss, g_train_op, d_train_op, samples, data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "reps = 20 \n",
    "ps = []\n",
    "for _ in tqdm(range(reps)):\n",
    "\n",
    "    loss, g_train_op, d_train_op, samples, data = initialize_omd_setup()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    xmax = 3\n",
    "    fs = []\n",
    "    frames = []\n",
    "    np_samples = []\n",
    "    n_batches_viz = 100\n",
    "    viz_every = params['viz_every']\n",
    "\n",
    "    #initialize at a very high loss value (to ensure that the first step is accepted)\n",
    "    f = [10000000]\n",
    "    disc_steps = 1\n",
    "    for i in tqdm(range(params['max_iter'])):\n",
    "\n",
    "\n",
    "        disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "        gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "\n",
    "\n",
    "        fs.append(f)\n",
    "\n",
    "\n",
    "        #take one generator gradient update and one discriminator gradient update\n",
    "        f_new, _, _ = sess.run([[loss, loss], g_train_op, d_train_op])\n",
    "\n",
    "        #take the additional discriminator gradient updates\n",
    "        for disc_steps  in range(0, disc_steps-1):\n",
    "             f_new, _ = sess.run([[loss], d_train_op])\n",
    "        f = f_new\n",
    "\n",
    "        # Gradient clipping for WGAN\n",
    "        for j in range(len(disc_vars)):\n",
    "            sess.run(tf.assign(disc_vars[j], tf.clip_by_value(disc_vars[j], clip_value_min=-0.1, clip_value_max=0.1)))\n",
    "\n",
    "        #plot the output\n",
    "        if i % viz_every == 0:\n",
    "            np_samples.append(np.vstack([sess.run(samples) for _ in range(n_batches_viz)]))\n",
    "            xx, yy = sess.run([samples, data])\n",
    "            fig = figure(figsize=(5,5))\n",
    "            ps.append((xx,yy))\n",
    "            scatter(yy[:, 0], yy[:, 1], c='r', edgecolor='none')\n",
    "            scatter(xx[:, 0], xx[:, 1], edgecolor='none')\n",
    "            axis('off')\n",
    "            plt.xlim(-1.5, 1.5)\n",
    "            plt.ylim(-1.5, 1.5)\n",
    "\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
